{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "C9dVOfvpydGJ"
      },
      "source": [
        "**GRU training notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmWJ3723yxtR"
      },
      "source": [
        "1. Import all the needed libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BBsUs8UODi78"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch import optim\n",
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "!pip install ipdb\n",
        "import ipdb\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import os, gc\n",
        "%matplotlib inline\n",
        "\n",
        "SEED = 0\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQhC920yybwy"
      },
      "source": [
        "2. Define neural architecture (gru_net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 288,
      "metadata": {
        "id": "BxQEmTYtEu9o"
      },
      "outputs": [],
      "source": [
        "class gru_net(nn.Module):\n",
        "    def __init__(self, model_pars, model_hpars, train_pars):\n",
        "        super(gru_net, self).__init__()\n",
        "        dropout_layer =  torch.nn.Dropout(p = model_hpars['p_dropout'])\n",
        "        self.dropout1 = dropout_layer\n",
        "        self.rnn      = torch.nn.GRU(input_size   = model_pars['n_inputs'],\n",
        "                                    hidden_size   = model_hpars['n_units'],\n",
        "                                    num_layers    = model_hpars['n_layers'],\n",
        "                                    batch_first   = True,\n",
        "                                    dropout       = (dropout_layer if model_hpars['n_layers'] > 1 else 0.),\n",
        "                                    bidirectional = False,\n",
        "                                    device        = train_pars.get('device', torch.device('CPU')),\n",
        "                                    dtype         = train_pars.get('dtype', None))\n",
        "        self.dropout2  = dropout_layer\n",
        "        self.transform = torch.nn.Linear(model_hpars['n_units'], model_pars['n_outputs'])\n",
        "    \n",
        "    def forward(self, X):\n",
        "        output, hidden = self.rnn(self.dropout1(X))\n",
        "        output         = self.transform(self.dropout2(output))\n",
        "        return F.log_softmax(output,dim = 2).swapaxes(1,2)\n",
        "        # before: n_batches X n_times X n_classes\n",
        "        # after: n_batches X n_classes X n_times"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GDWcffVxS1kY"
      },
      "source": [
        "3. Define class (GRUmodel) and methods to support data preparation, training, evaluation, and plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 289,
      "metadata": {
        "id": "BVXwRnqtqzIs"
      },
      "outputs": [],
      "source": [
        "class GRUmodel:\n",
        "    \"\"\"Class that handles operations on gru_net\"\"\"\n",
        "    def __init__(self, data, model_init, model_pars, model_hpars, train_pars, log_pars):\n",
        "        self.model      = model_init(model_pars, model_hpars, train_pars)\n",
        "        self.model_pars = model_pars\n",
        "        self.data       = data\n",
        "        self.train_pars = train_pars\n",
        "        self.log_pars   = log_pars\n",
        "        self.create_DataLoaders()\n",
        "        if self.train_pars['use_gpu'] and torch.cuda.is_available():\n",
        "            self.device = torch.device('cuda:0')\n",
        "            gpu_idxs    = np.arange(min(train_pars['n_gpus'], torch.cuda.device_count())).tolist()\n",
        "            self.model  = torch.nn.DataParallel(self.model.to(self.device), device_ids = gpu_idxs)\n",
        "        else:\n",
        "          self.device = 'cpu'\n",
        "        self.optimizer  = torch.optim.Adam(self.model.parameters(), \n",
        "                                          lr           = train_pars.get('lrate', 1e-3), \n",
        "                                          weight_decay = train_pars.get('alpha', 0.0))\n",
        "        self.loss        = nn.NLLLoss() \n",
        "        self.trainLoss, self.trainAcc = [], []\n",
        "        self.validLoss, self.validAcc  = [], []\n",
        "        if self.log_pars['save_checkpoints']:\n",
        "          if not os.path.isdir(self.log_pars['save_path']):\n",
        "            os.mkdir(self.log_pars['save_path'])\n",
        "        self.last_improv = 0\n",
        "        self.results = {'acc':{},'loss':{},'best_epoch':{}}\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"Trains model for given number of iterations with early stopping\"\"\"\n",
        "        best_valid_loss = 1e8\n",
        "        last_improv     = -1\n",
        "        print('Training Started\\n')\n",
        "        for i in range(self.train_pars['n_epochs']):\n",
        "            print(f\"Epoch: {i+1}/{self.train_pars['n_epochs']}\")\n",
        "\n",
        "            # Train on entire dataset\n",
        "            train_loss, train_acc    = self.train_epoch()\n",
        "            valid_loss, valid_acc, _ = self.eval_epoch('validation')\n",
        "\n",
        "            # Store train and validation losses\n",
        "            self.trainLoss.append(train_loss)\n",
        "            self.trainAcc.append(train_acc)\n",
        "            self.validLoss.append(valid_loss)\n",
        "            self.validAcc.append(valid_acc)\n",
        "\n",
        "            # Print train and validation losses\n",
        "            if self.log_pars['verbose']:\n",
        "                if (i % log_pars['print_freq']) == 0:\n",
        "                    print(f\"Training  : loss = {train_loss:.2f}, acc = {train_acc:.2f}\")\n",
        "                    print(f\"Validation: loss = {valid_loss:.2f}, acc = {valid_acc:.2f}\")\n",
        "\n",
        "            # Early stopping\n",
        "            if valid_loss < best_valid_loss:\n",
        "                best_valid_loss  = valid_loss\n",
        "                self.last_improv = i\n",
        "                if self.log_pars['save_checkpoints'] is not None:\n",
        "                    self.save_checkpoint(best_valid_loss, valid_acc, self.last_improv)\n",
        "            if (i - self.last_improv) > train_pars['valid_pat']:\n",
        "                break\n",
        "\n",
        "        # Store final results        \n",
        "        self.results['acc']['train']  = self.trainAcc[self.last_improv]\n",
        "        self.results['loss']['train'] = self.trainLoss[self.last_improv]\n",
        "        self.results['acc']['valid']  = self.validAcc[self.last_improv]\n",
        "        self.results['loss']['valid'] = self.validLoss[self.last_improv]\n",
        "        self.results['best_epoch']    = self.last_improv\n",
        "        self.plot_training_results()\n",
        "        return self.results\n",
        "\n",
        "    def train_epoch(self):\n",
        "        # Train the model for one epoch\n",
        "        num_batches = len(self.DataLoaderTrain)\n",
        "        num_samples = num_batches*self.train_pars['batch_size']*self.model_pars['n_times']\n",
        "        train_loss, train_acc = 0, 0\n",
        "        self.model.train()\n",
        "        for batch, (X, y) in enumerate(self.DataLoaderTrain):\n",
        "            X, y = X.to(self.device), y.to(self.device)\n",
        "\n",
        "            # Compute prediction error\n",
        "            ypred        = self.model(X)\n",
        "            c_train_loss = self.loss(ypred, y)\n",
        "\n",
        "            # Zero the gradient\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            # Calculate gradient\n",
        "            c_train_loss.backward()\n",
        "\n",
        "            # Update weights\n",
        "            self.optimizer.step()\n",
        "            \n",
        "            # Acumulate losses and correct classifications\n",
        "            train_loss += c_train_loss.item()*self.model_pars['n_times']\n",
        "            train_acc  += (ypred.argmax(1) == y).type(torch.float).sum().item()\n",
        "        train_loss /= num_samples\n",
        "        train_acc  /= num_samples\n",
        "        return train_loss, train_acc\n",
        "\n",
        "    def eval_epoch(self, modality):\n",
        "       # Evaluate the model\n",
        "        if modality=='validation':\n",
        "          dataloader = self.DataLoaderVal\n",
        "        elif modality=='test':\n",
        "          dataloader = self.DataLoaderTest\n",
        "\n",
        "        num_batches = len(dataloader)\n",
        "        num_samples = num_batches*self.train_pars['batch_size']*self.model_pars['n_times']\n",
        "        eval_loss, eval_acc = 0, 0\n",
        "        self.model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "          for batch, (X, y) in enumerate(dataloader):\n",
        "            X, y  = X.to(self.device), y.to(self.device)\n",
        "            ypred = self.model(X)\n",
        "            eval_loss += self.loss(ypred, y).item()*self.model_pars['n_times']\n",
        "            eval_acc  += (ypred.argmax(1) == y).type(torch.float).sum().item()\n",
        "          eval_loss /= num_samples\n",
        "          eval_acc  /= num_samples\n",
        "          return eval_loss, eval_acc, ypred\n",
        "\n",
        "    def test(self): \n",
        "      # Evaluate the model on test set\n",
        "      self.results['loss']['test'], self.results['acc']['test'], ypred = self.eval_epoch('test')\n",
        "      return self.results, ypred\n",
        "\n",
        "    def plot_training_results(self):\n",
        "      # Plot training results\n",
        "      last_index = self.last_improv+1\n",
        "      neps = range(0,last_index)\n",
        "      plt.plot(neps, self.trainLoss[0:last_index], label = 'training loss')\n",
        "      plt.plot(neps, self.trainAcc[0:last_index], label = 'training acc')\n",
        "      plt.plot(neps, self.validLoss[0:last_index], label = 'validation loss')\n",
        "      plt.plot(neps, self.validAcc[0:last_index], label  = 'validation acc')\n",
        "      plt.xlabel('Training Epochs')\n",
        "      plt.legend()\n",
        "      plt.title(f'Training Results[0:{self.last_improv}]')\n",
        "      plt.show()\n",
        "\n",
        "    def create_DataLoaders(self):\n",
        "      # Create data loaders to support batch training\n",
        "      self.DataLoaderTrain =        DataLoader(TensorDataset(self.data['train']['X'], \n",
        "                                    self.data['train']['y']), \n",
        "                                    batch_size = self.train_pars['batch_size'], \n",
        "                                    shuffle    = True)\n",
        "      self.DataLoaderVal =          DataLoader(TensorDataset(self.data['valid']['X'], \n",
        "                                    self.data['valid']['y']), \n",
        "                                    batch_size = self.train_pars['batch_size'])\n",
        "      self.DataLoaderTest =         DataLoader(TensorDataset(self.data['test']['X'], \n",
        "                                    self.data['test']['y']), \n",
        "                                    batch_size = self.train_pars['batch_size']) \n",
        "\n",
        "    def save_checkpoint(self, best_valid_loss, valid_acc, last_improv):\n",
        "      # Save model checkpoint\n",
        "        default_ckpt = {\n",
        "            \"state_dict\"  : self.model.state_dict(),\n",
        "            \"optim_state\" : self.optimizer.state_dict(),\n",
        "            \"valid_loss\"  : best_valid_loss,\n",
        "            \"valid_acc\"   : valid_acc,\n",
        "            \"epoch\"       : last_improv\n",
        "        }\n",
        "        file_name = self.log_pars['save_path'] + f'epoch_{last_improv}_valLoss_{best_valid_loss}_valAcc_{valid_acc}'\n",
        "        if not os.path.exists(file_name):\n",
        "          torch.save(default_ckpt, file_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzh2VQvb1nrh"
      },
      "source": [
        "4. Load and prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 291,
      "metadata": {
        "id": "Mr8EAwvkaQ_C"
      },
      "outputs": [],
      "source": [
        "# Define function to convert data to torch tensors\n",
        "def tensorize_dict(data_in):\n",
        "  for phase in data_in:\n",
        "   data_in[phase]['X'] = torch.from_numpy(data_in[phase]['X']).to(torch.float32)\n",
        "   data_in[phase]['y'] = torch.from_numpy(data_in[phase]['y']).to(torch.long)\n",
        "  return data_in\n",
        "# Load the preprocessed\n",
        "data_ts = np.load('./preprocessed/ppd.npy',      allow_pickle='TRUE').item()\n",
        "dims_ts = np.load('./preprocessed/ppd_dims.npy', allow_pickle='TRUE').item()\n",
        "data_ts = tensorize_dict(data_ts)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CPr08foh294L"
      },
      "source": [
        "5. Initialize the GRUmodel class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsasxR4V1o8E"
      },
      "outputs": [],
      "source": [
        "# Define path to store intermediate results\n",
        "SESSION_NAME         = datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\") + '_GRU/'\n",
        "PATH_TO_FILES_DIR    = PATH_TO_DRIVE + 'MyFolder/' + SESSION_NAME\n",
        "\n",
        "# Define parameters\n",
        "model_pars     = {'n_inputs': dims_ts['inp'], 'n_outputs': 2*dims_ts['outp'],\\\n",
        "                  'n_times': dims_ts['times']}\n",
        "model_hpars    = {'n_units': 400, 'n_layers':1, 'p_dropout': 0.25,\n",
        "                  'lrate': 1e-3, 'l2_weight':0}\n",
        "train_pars     = {'n_epochs': 2000, 'batch_size': 1, 'valid_pat': 50,\\\n",
        "                  'use_gpu': True, 'n_gpus':1}\n",
        "log_pars       = {'print_freq': 1, 'save_checkpoints': True, 'save_path': PATH_TO_FILES_DIR, \n",
        "                  'verbose': True}\n",
        "\n",
        "# Initialize the model\n",
        "model = GRUmodel(\n",
        "    data        = data_ts,\n",
        "    model_init  = gru_net,\n",
        "    model_pars  = model_pars,\n",
        "    model_hpars = model_hpars,\n",
        "    train_pars  = train_pars,\n",
        "    log_pars    = log_pars\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QeL2bOHD3mfv"
      },
      "source": [
        "6. Train and test the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lA8WNFM-3qca"
      },
      "outputs": [],
      "source": [
        "# Train the model and inspect results\n",
        "model.train()\n",
        "\n",
        "# Test the model\n",
        "results, predictions = model.test()\n",
        "results['test_pred'] = predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 294,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMahhYOWq548",
        "outputId": "134a8995-1f01-4a3f-a8de-cf88f3ebd159"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'acc': {'train': 0.82525, 'valid': 0.8245, 'test': 0.82575},\n",
              " 'loss': {'train': 0.3912906050682068,\n",
              "  'valid': 0.3853456974029541,\n",
              "  'test': 0.6048155426979065},\n",
              " 'best_epoch': 33,\n",
              " 'test_pred': tensor([[[-1.7276e-03, -1.9715e-04, -1.7248e-04,  ..., -3.7792e-01,\n",
              "           -6.6707e-03, -1.2265e-03],\n",
              "          [-6.3619e+00, -8.5318e+00, -8.6651e+00,  ..., -1.1561e+00,\n",
              "           -5.0134e+00, -6.7042e+00]]], device='cuda:0')}"
            ]
          },
          "execution_count": 294,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "eeb623c35880e6780fd8bc2c64a58edb45a80b0c5d6f61e099cca68ddbb55637"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
